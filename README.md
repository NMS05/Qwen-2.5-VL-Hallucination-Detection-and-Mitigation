# Qwen 2.5 VL - Hallucination Detection and Mitigation

## Overview

This repository contains code for detecting and mitigating hallucinations in AI generated responses using Qwen models. It utilizes the Qwen 2.5 VL 7B model to identify hallucinations and subsequently rectify them.

## Dataset

This project uses a filtered version of the [POVID dataset](https://github.com/YiyangZhou/POVID), stored in `filtered_povid.json`. This dataset includes:
* ~1k captioning samples (2-5 sentence response) + ~1k VQA samples (1-2 sentence response)
* Each sample contains only the hallucinatory response from the original POVID dataset responses.
* In POVID, hallucinatory responses were generated by instructing ChatGPT to injecting errors into the correct (ground-truth) response. 

Our objective is to detect these errors (HAL detection) and rectify them (HAL mitigation) using Qwen models!  

## Methodology

The process involves several steps:

1.  **Claim Extraction:** Claims are extracted from (already hallucinated) responses using a Qwen-2.5-7B-Instruct LLM (smaller models like 3B sometimes dosen't follow the instruction properly). This is handled within `qwen_wrapper.py`.
2.  **Hallucination Annotation:** The `qwen_HAL_annotator.py` script leverages the Qwen VL model to annotate each extracted claim. Qwen-2.5-VL-7B-Instruct is used currently; depending on your GPU resources, please use larger/powerful models for more reliable annotations. Annotations include:
    * **Label:** Hallucination or Non-hallucination.
    * **Reason:** An explanation for the annotation, aiding interpretability and error correction.
3.  **Response Rectification:** Based on the annotations, the `qwen_HAL_annotator.py` script attempts to rectify the errors found in the original response.

**Analysis:** The `analyse_qwen_annotations.py` script can be used to visualize the annotation results.

### Key Files

* `filtered_povid.json`: A sub-set of POVID dataset, containing hallucinatory samples.
* `qwen_wrapper.py`: Contains wrappers for the Qwen 2.5 LLM and Qwen 2.5 VL models.
* `qwen_HAL_annotator.py`: Core script for claim extraction, annotation, and rectification.
* `analyse_qwen_annotations.py`: Script for visualizing annotation results.
* `Qwen_HAL_Annotations.json`: Output annotations for 100 random samples for your reference!

## Current Status

**Work in Progress:** This repository is under active development. 

* The current Qwen-HAL-Detection pipeline is suited ONLY for image captioning and single-round VQA tasks.
* Updates as of 8 April,
    - LLM: Qwen 2.5 14B Instruct --> Qwen 2.5 7B Instruct
    - Refined the prompts (removed subjective label as it is noisy)
    - Removed annotation self-verification step (seemed unnecessary + reduces latency)
    - IMPORTANT: Qwen-VL now evaluates each and every claim independently and NOT all-at-once (though this is faster, the annotation quality is not that good!)
